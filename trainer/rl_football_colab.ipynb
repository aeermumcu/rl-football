{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ RL Football - Fast GPU Training\n\n",
                "**Vectorized trainer with 16 parallel environments**\n\n",
                "1. Set Runtime ‚Üí GPU (T4)\n",
                "2. Run all cells\n",
                "3. Download `trained.json` when done\n\n",
                "**Expected speed: 15-30 episodes/sec on T4**"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Setup & GPU check\n",
                "import os\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import json\n",
                "import time\n",
                "\n",
                "print(f'TensorFlow: {tf.__version__}')\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "print(f'GPU: {gpus}')\n",
                "if gpus:\n",
                "    print('‚úÖ GPU detected - training will be fast!')\n",
                "else:\n",
                "    print('‚ö†Ô∏è No GPU - go to Runtime ‚Üí Change runtime type ‚Üí GPU')"
            ],
            "metadata": {
                "id": "setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Vectorized Game - runs N parallel environments with NumPy\n",
                "class VectorizedGame:\n",
                "    def __init__(self, n_envs=16):\n",
                "        self.n = n_envs\n",
                "        self.W, self.H = 720, 420\n",
                "        self.reset_all()\n",
                "\n",
                "    def reset_all(self):\n",
                "        self.blip = np.tile([120., 210., 0., 0.], (self.n, 1))\n",
                "        self.bloop = np.tile([600., 210., 0., 0.], (self.n, 1))\n",
                "        self.ball = np.tile([360., 210., 0., 0.], (self.n, 1))\n",
                "        self.scores = np.zeros((self.n, 2), dtype=np.int32)\n",
                "        self.time = np.full(self.n, 30.0)\n",
                "        self.kick_flags = np.zeros((self.n, 2), dtype=np.int32)\n",
                "        self.done = np.zeros(self.n, dtype=bool)\n",
                "\n",
                "    def reset_positions(self, mask):\n",
                "        self.blip[mask, :2] = [120., 210.]\n",
                "        self.bloop[mask, :2] = [600., 210.]\n",
                "        self.ball[mask] = [360., 210., 0., 0.]\n",
                "\n",
                "    def step(self, actions_blip, actions_bloop):\n",
                "        MOVES = np.array([[0,-1],[0,1],[-1,0],[1,0],[-1,-1],[1,-1],[-1,1],[1,1],[0,0],[0,0]], dtype=np.float32)\n",
                "\n",
                "        for i, (player, actions) in enumerate([(self.blip, actions_blip), (self.bloop, actions_bloop)]):\n",
                "            move_mask = actions < 8\n",
                "            if np.any(move_mask):\n",
                "                dirs = MOVES[actions[move_mask]]\n",
                "                player[move_mask, 2] += dirs[:, 0] * 2\n",
                "                player[move_mask, 3] += dirs[:, 1] * 2\n",
                "            self.kick_flags[:, i] = (actions == 8).astype(np.int32)\n",
                "            speed = np.sqrt(player[:, 2]**2 + player[:, 3]**2)\n",
                "            too_fast = speed > 4\n",
                "            if np.any(too_fast):\n",
                "                player[too_fast, 2] *= 4.0 / speed[too_fast]\n",
                "                player[too_fast, 3] *= 4.0 / speed[too_fast]\n",
                "\n",
                "        for player in [self.blip, self.bloop]:\n",
                "            player[:, 0] += player[:, 2]\n",
                "            player[:, 1] += player[:, 3]\n",
                "            player[:, 2:4] *= 0.85\n",
                "            player[:, 0] = np.clip(player[:, 0], 25, self.W - 25)\n",
                "            player[:, 1] = np.clip(player[:, 1], 25, self.H - 25)\n",
                "\n",
                "        self.ball[:, 0] += self.ball[:, 2]\n",
                "        self.ball[:, 1] += self.ball[:, 3]\n",
                "        self.ball[:, 2:4] *= 0.98\n",
                "\n",
                "        # Wall bounces\n",
                "        top = self.ball[:, 1] < 12\n",
                "        self.ball[top, 1] = 12\n",
                "        self.ball[top, 3] *= -0.8\n",
                "        bot = self.ball[:, 1] > self.H - 12\n",
                "        self.ball[bot, 1] = self.H - 12\n",
                "        self.ball[bot, 3] *= -0.8\n",
                "\n",
                "        goal_y_min, goal_y_max = 150, 270\n",
                "        in_goal = (self.ball[:, 1] > goal_y_min) & (self.ball[:, 1] < goal_y_max)\n",
                "        left = (self.ball[:, 0] < 12) & ~in_goal\n",
                "        self.ball[left, 0] = 12\n",
                "        self.ball[left, 2] *= -0.8\n",
                "        right = (self.ball[:, 0] > self.W - 12) & ~in_goal\n",
                "        self.ball[right, 0] = self.W - 12\n",
                "        self.ball[right, 2] *= -0.8\n",
                "\n",
                "        # Player-ball collisions\n",
                "        for i, player in enumerate([self.blip, self.bloop]):\n",
                "            dx = self.ball[:, 0] - player[:, 0]\n",
                "            dy = self.ball[:, 1] - player[:, 1]\n",
                "            dist = np.sqrt(dx**2 + dy**2)\n",
                "            collide = (dist > 0) & (dist < 37)\n",
                "            if np.any(collide):\n",
                "                nx = dx[collide] / dist[collide]\n",
                "                ny = dy[collide] / dist[collide]\n",
                "                self.ball[collide, 0] = player[collide, 0] + nx * 37\n",
                "                self.ball[collide, 1] = player[collide, 1] + ny * 37\n",
                "                power = np.where(self.kick_flags[collide, i], 12, 6)\n",
                "                self.ball[collide, 2] = nx * power + player[collide, 2] * 0.5\n",
                "                self.ball[collide, 3] = ny * power + player[collide, 3] * 0.5\n",
                "        self.kick_flags[:] = 0\n",
                "\n",
                "        # Goals\n",
                "        events = np.full(self.n, None, dtype=object)\n",
                "        bloop_goal = in_goal & (self.ball[:, 0] < 0)\n",
                "        if np.any(bloop_goal):\n",
                "            self.scores[bloop_goal, 1] += 1\n",
                "            events[bloop_goal] = 'L'\n",
                "            self.reset_positions(bloop_goal)\n",
                "        blip_goal = in_goal & (self.ball[:, 0] > self.W)\n",
                "        if np.any(blip_goal):\n",
                "            self.scores[blip_goal, 0] += 1\n",
                "            events[blip_goal] = 'W'\n",
                "            self.reset_positions(blip_goal)\n",
                "\n",
                "        self.time -= 1/60\n",
                "        self.done = self.time <= 0\n",
                "        return events, self.done.copy()\n",
                "\n",
                "    def get_states(self, team=0):\n",
                "        p, o = (self.blip, self.bloop) if team == 0 else (self.bloop, self.blip)\n",
                "        dist = np.sqrt((p[:, 0] - self.ball[:, 0])**2 + (p[:, 1] - self.ball[:, 1])**2) / 830\n",
                "        return np.stack([\n",
                "            p[:, 0]/self.W, p[:, 1]/self.H, self.ball[:, 0]/self.W, self.ball[:, 1]/self.H,\n",
                "            np.clip(self.ball[:, 2]/15, -1, 1), np.clip(self.ball[:, 3]/15, -1, 1),\n",
                "            o[:, 0]/self.W, o[:, 1]/self.H, dist, np.zeros(self.n), np.zeros(self.n), np.zeros(self.n)\n",
                "        ], axis=1).astype(np.float32)\n",
                "\n",
                "print('‚úÖ VectorizedGame ready')"
            ],
            "metadata": {
                "id": "game"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Simple AI opponent (vectorized)\n",
                "def simple_ai_actions(states):\n",
                "    n = states.shape[0]\n",
                "    actions = np.full(n, 9, dtype=np.int32)\n",
                "    px, py, bx, by = states[:, 0], states[:, 1], states[:, 2], states[:, 3]\n",
                "    dist = states[:, 8]\n",
                "    dx, dy = bx - px, by - py\n",
                "\n",
                "    actions[dist < 0.04] = 8  # Kick\n",
                "    chase = dist >= 0.04\n",
                "\n",
                "    mx = np.zeros(n, dtype=np.int32)\n",
                "    my = np.zeros(n, dtype=np.int32)\n",
                "    mx[dx > 0.02] = 1\n",
                "    mx[dx < -0.02] = -1\n",
                "    my[dy > 0.02] = 1\n",
                "    my[dy < -0.02] = -1\n",
                "\n",
                "    action_map = {(0,-1):0, (0,1):1, (-1,0):2, (1,0):3, (-1,-1):4, (1,-1):5, (-1,1):6, (1,1):7}\n",
                "    for i in range(n):\n",
                "        if chase[i] and (mx[i], my[i]) in action_map:\n",
                "            actions[i] = action_map[(mx[i], my[i])]\n",
                "    return actions\n",
                "\n",
                "print('‚úÖ SimpleAI ready')"
            ],
            "metadata": {
                "id": "ai"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Dueling DQN model\n",
                "def create_model(lr=0.0005):\n",
                "    inp = layers.Input(shape=(12,))\n",
                "    x = layers.Dense(256, activation='relu', kernel_initializer='he_normal')(inp)\n",
                "    x = layers.Dense(256, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    x = layers.Dense(128, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    v = layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    v = layers.Dense(1, kernel_initializer='he_normal')(v)\n",
                "    a = layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    a = layers.Dense(10, kernel_initializer='he_normal')(a)\n",
                "    m = layers.Lambda(lambda t: tf.reduce_mean(t, axis=1, keepdims=True))(a)\n",
                "    q = layers.Add()([v, layers.Subtract()([a, m])])\n",
                "    model = keras.Model(inp, q)\n",
                "    model.compile(optimizer=keras.optimizers.Adam(lr), loss='mse')\n",
                "    return model\n",
                "\n",
                "# Compiled TensorFlow functions for speed\n",
                "@tf.function\n",
                "def predict_batch(model, states):\n",
                "    return model(states, training=False)\n",
                "\n",
                "@tf.function\n",
                "def train_step(model, optimizer, states, targets):\n",
                "    with tf.GradientTape() as tape:\n",
                "        q = model(states, training=True)\n",
                "        loss = tf.reduce_mean(tf.square(q - targets))\n",
                "    grads = tape.gradient(loss, model.trainable_variables)\n",
                "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
                "    return loss\n",
                "\n",
                "print('‚úÖ DQN model ready')"
            ],
            "metadata": {
                "id": "model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Efficient replay buffer\n",
                "class ReplayBuffer:\n",
                "    def __init__(self, cap=50000):\n",
                "        self.cap = cap\n",
                "        self.states = np.zeros((cap, 12), dtype=np.float32)\n",
                "        self.actions = np.zeros(cap, dtype=np.int32)\n",
                "        self.rewards = np.zeros(cap, dtype=np.float32)\n",
                "        self.next_states = np.zeros((cap, 12), dtype=np.float32)\n",
                "        self.dones = np.zeros(cap, dtype=bool)\n",
                "        self.idx = 0\n",
                "        self.size = 0\n",
                "\n",
                "    def add_batch(self, s, a, r, s2, d):\n",
                "        for i in range(len(s)):\n",
                "            self.states[self.idx] = s[i]\n",
                "            self.actions[self.idx] = a[i]\n",
                "            self.rewards[self.idx] = r[i]\n",
                "            self.next_states[self.idx] = s2[i]\n",
                "            self.dones[self.idx] = d[i]\n",
                "            self.idx = (self.idx + 1) % self.cap\n",
                "            self.size = min(self.size + 1, self.cap)\n",
                "\n",
                "    def sample(self, batch_size):\n",
                "        idx = np.random.choice(self.size, batch_size, replace=False)\n",
                "        return self.states[idx], self.actions[idx], self.rewards[idx], self.next_states[idx], self.dones[idx]\n",
                "\n",
                "# Reward calculation\n",
                "def calc_rewards(game, events, last_d):\n",
                "    p, b, o = game.blip, game.ball, game.bloop\n",
                "    d = np.sqrt((p[:, 0] - b[:, 0])**2 + (p[:, 1] - b[:, 1])**2)\n",
                "    r = np.zeros(game.n, dtype=np.float32)\n",
                "    r[events == 'W'] += 500\n",
                "    r[events == 'L'] -= 300\n",
                "    r += (1 - d/830) * 5\n",
                "    r[d < 40] += 10\n",
                "    if last_d is not None:\n",
                "        delta = last_d - d\n",
                "        r += delta * 0.5\n",
                "        r[delta > 2] += 3\n",
                "    sp = np.sqrt(p[:, 2]**2 + p[:, 3]**2)\n",
                "    r[(sp < 0.5) & (d > 50)] -= 8\n",
                "    r[sp > 1] += 1\n",
                "    r[(b[:, 2] > 2) & (d < 80)] += 8\n",
                "    r[np.abs(b[:, 0] - 720) < 100] += 5\n",
                "    corner = ((p[:, 0] < 80) | (p[:, 0] > 640)) & ((p[:, 1] < 80) | (p[:, 1] > 340))\n",
                "    r[corner] -= 5\n",
                "    r[corner & (d > 100)] -= 5\n",
                "    r[d > 300] -= 5\n",
                "    r[(d > 200) & (d <= 300)] -= 3\n",
                "    r[(d > 150) & (d <= 200)] -= 1\n",
                "    od = np.sqrt((o[:, 0] - b[:, 0])**2 + (o[:, 1] - b[:, 1])**2)\n",
                "    r[(od < d) & (d > 60)] -= 2\n",
                "    r -= 0.1\n",
                "    return r, d\n",
                "\n",
                "print('‚úÖ Buffer & rewards ready')"
            ],
            "metadata": {
                "id": "buffer"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Training configuration\n",
                "EPISODES = 100000  # Change this for shorter/longer training\n",
                "N_ENVS = 32        # Parallel environments (increase for faster training)\n",
                "BATCH_SIZE = 64\n",
                "GAMMA = 0.995\n",
                "EPS_DECAY = 0.9999\n",
                "EPS_MIN = 0.02\n",
                "TARGET_UPDATE = 500\n",
                "SAVE_EVERY = 10000\n",
                "\n",
                "print(f'Training {EPISODES} episodes with {N_ENVS} parallel environments')"
            ],
            "metadata": {
                "id": "config"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Main training loop\n",
                "def train():\n",
                "    print('='*60)\n",
                "    print('üöÄ Starting GPU-accelerated training')\n",
                "    print('='*60)\n",
                "\n",
                "    game = VectorizedGame(N_ENVS)\n",
                "    model = create_model()\n",
                "    target = create_model()\n",
                "    target.set_weights(model.get_weights())\n",
                "    optimizer = keras.optimizers.Adam(0.0005)\n",
                "    buffer = ReplayBuffer(50000)\n",
                "\n",
                "    eps = 1.0\n",
                "    step_count = 0\n",
                "    stats = {'W': 0, 'L': 0, 'D': 0, 'goals': 0}\n",
                "    t0 = time.time()\n",
                "    total_ep = 0\n",
                "\n",
                "    while total_ep < EPISODES:\n",
                "        game.reset_all()\n",
                "        last_d = None\n",
                "\n",
                "        while not np.all(game.done):\n",
                "            active = ~game.done\n",
                "            s1 = game.get_states(0)\n",
                "            s2 = game.get_states(1)\n",
                "\n",
                "            # Action selection\n",
                "            a1 = np.zeros(game.n, dtype=np.int32)\n",
                "            rand = np.random.random(game.n) < eps\n",
                "            a1[rand] = np.random.randint(0, 10, size=np.sum(rand))\n",
                "            if np.any(~rand):\n",
                "                q = predict_batch(model, s1[~rand]).numpy()\n",
                "                a1[~rand] = np.argmax(q, axis=1)\n",
                "            a2 = simple_ai_actions(s2)\n",
                "\n",
                "            events, dones = game.step(a1, a2)\n",
                "            rewards, last_d = calc_rewards(game, events, last_d)\n",
                "            new_s = game.get_states(0)\n",
                "\n",
                "            buffer.add_batch(s1[active], a1[active], rewards[active], new_s[active], dones[active])\n",
                "            stats['goals'] += np.sum(events != None)\n",
                "\n",
                "            # Train\n",
                "            if buffer.size >= 500:\n",
                "                s, a, r, s2, d = buffer.sample(BATCH_SIZE)\n",
                "                q_main = predict_batch(model, s2).numpy()\n",
                "                q_target = predict_batch(target, s2).numpy()\n",
                "                best = np.argmax(q_main, axis=1)\n",
                "                next_q = q_target[np.arange(BATCH_SIZE), best]\n",
                "\n",
                "                targets = predict_batch(model, s).numpy()\n",
                "                for i in range(BATCH_SIZE):\n",
                "                    targets[i, a[i]] = r[i] if d[i] else r[i] + GAMMA * next_q[i]\n",
                "\n",
                "                train_step(model, optimizer, tf.constant(s), tf.constant(targets))\n",
                "                step_count += 1\n",
                "\n",
                "                if step_count % TARGET_UPDATE == 0:\n",
                "                    target.set_weights(model.get_weights())\n",
                "\n",
                "        # Episode end\n",
                "        for i in range(game.n):\n",
                "            if game.scores[i, 0] > game.scores[i, 1]: stats['W'] += 1\n",
                "            elif game.scores[i, 1] > game.scores[i, 0]: stats['L'] += 1\n",
                "            else: stats['D'] += 1\n",
                "\n",
                "        total_ep += N_ENVS\n",
                "        for _ in range(N_ENVS):\n",
                "            eps = max(EPS_MIN, eps * EPS_DECAY)\n",
                "\n",
                "        # Progress\n",
                "        if total_ep % (100 * N_ENVS) < N_ENVS or total_ep <= N_ENVS:\n",
                "            el = time.time() - t0\n",
                "            sp = total_ep / el if el > 0 else 0\n",
                "            eta = (EPISODES - total_ep) / sp if sp > 0 else 0\n",
                "            eta_s = f'{eta/3600:.1f}h' if eta > 3600 else f'{eta/60:.1f}m'\n",
                "            print(f'Ep {total_ep}/{EPISODES} | Œµ:{eps:.4f} | W:{stats[\"W\"]} L:{stats[\"L\"]} D:{stats[\"D\"]} | {sp:.1f}/s | ETA:{eta_s}')\n",
                "\n",
                "        # Save checkpoint\n",
                "        if total_ep % SAVE_EVERY < N_ENVS:\n",
                "            save_weights(model, eps, step_count, f'weights_{total_ep}.json')\n",
                "\n",
                "    # Final save\n",
                "    save_weights(model, eps, step_count, 'trained.json')\n",
                "    print('='*60)\n",
                "    print('‚úÖ Training complete!')\n",
                "    print(f'Final: W:{stats[\"W\"]} L:{stats[\"L\"]} D:{stats[\"D\"]} | Goals:{stats[\"goals\"]}')\n",
                "    print(f'Time: {(time.time()-t0)/3600:.2f} hours')\n",
                "    print('='*60)\n",
                "    return model, stats\n",
                "\n",
                "def save_weights(model, eps, steps, filename):\n",
                "    weights = [{'shape': list(w.shape), 'data': w.flatten().tolist()} for w in model.get_weights()]\n",
                "    data = {\n",
                "        'version': 2, 'aiType': 'dqn',\n",
                "        'blipAgent': {'weights': weights, 'epsilon': float(eps), 'trainStepCount': int(steps)},\n",
                "        'bloopAgent': {'weights': weights, 'epsilon': float(eps), 'trainStepCount': int(steps)},\n",
                "        'blip': {'weights': weights, 'epsilon': float(eps), 'trainStepCount': int(steps)},\n",
                "        'bloop': {'weights': weights, 'epsilon': float(eps), 'trainStepCount': int(steps)}\n",
                "    }\n",
                "    with open(filename, 'w') as f:\n",
                "        json.dump(data, f)\n",
                "    print(f'üíæ Saved {filename}')\n",
                "\n",
                "print('‚úÖ Training function ready')"
            ],
            "metadata": {
                "id": "train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# üöÄ START TRAINING\n",
                "model, stats = train()"
            ],
            "metadata": {
                "id": "run"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download trained weights\n",
                "from google.colab import files\n",
                "files.download('trained.json')"
            ],
            "metadata": {
                "id": "download"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}