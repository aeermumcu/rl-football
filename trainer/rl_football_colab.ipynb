{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸš€ RL Football - GPU Training v2\n\n",
                "**Optimized for T4 GPU with 64 parallel environments**\n\n",
                "1. Runtime â†’ GPU (T4) âœ“\n",
                "2. Run all cells\n",
                "3. Download `trained.json` when done\n\n",
                "**Expected: 100k episodes in ~2-4 hours**"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Setup\n",
                "import os\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import json, time\n",
                "\n",
                "print(f'TensorFlow: {tf.__version__}')\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "print(f'GPU: {gpus}')\n",
                "if not gpus:\n",
                "    print('âš ï¸ No GPU! Go to Runtime â†’ Change runtime type â†’ T4 GPU')\n",
                "else:\n",
                "    print('âœ… GPU ready')"
            ],
            "metadata": {
                "id": "setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================\n",
                "# CONFIGURATION - Adjust these for your needs\n",
                "# ============================================================\n",
                "EPISODES = 100000     # Total episodes to train\n",
                "N_ENVS = 64           # Parallel environments (64 is good for T4)\n",
                "BATCH_SIZE = 128      # Larger batch = better GPU utilization\n",
                "TRAIN_EVERY = 2       # Train every N steps (lower = more training)\n",
                "SAVE_EVERY = 10000    # Save checkpoint every N episodes\n",
                "\n",
                "# Hyperparameters (optimized)\n",
                "GAMMA = 0.995\n",
                "LR = 0.0005\n",
                "EPS_DECAY = 0.9999\n",
                "EPS_MIN = 0.02\n",
                "TARGET_UPDATE = 500\n",
                "BUFFER_SIZE = 100000\n",
                "\n",
                "print(f'Config: {EPISODES} episodes, {N_ENVS} parallel envs, batch {BATCH_SIZE}')"
            ],
            "metadata": {
                "id": "config"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================\n",
                "# VECTORIZED GAME ENGINE\n",
                "# ============================================================\n",
                "class Game:\n",
                "    \"\"\"Runs N football games in parallel using pure NumPy.\"\"\"\n",
                "    \n",
                "    def __init__(self, n):\n",
                "        self.n = n\n",
                "        self.W, self.H = 720, 420\n",
                "        # Pre-compute movement vectors\n",
                "        self.MOVES = np.array([\n",
                "            [0,-1], [0,1], [-1,0], [1,0],\n",
                "            [-1,-1], [1,-1], [-1,1], [1,1],\n",
                "            [0,0], [0,0]\n",
                "        ], dtype=np.float32)\n",
                "        self.reset()\n",
                "    \n",
                "    def reset(self):\n",
                "        \"\"\"Reset all environments.\"\"\"\n",
                "        self.p1 = np.zeros((self.n, 4), dtype=np.float32)  # x, y, vx, vy\n",
                "        self.p2 = np.zeros((self.n, 4), dtype=np.float32)\n",
                "        self.ball = np.zeros((self.n, 4), dtype=np.float32)\n",
                "        self.p1[:, :2] = [120, 210]\n",
                "        self.p2[:, :2] = [600, 210]\n",
                "        self.ball[:, :2] = [360, 210]\n",
                "        self.scores = np.zeros((self.n, 2), dtype=np.int32)\n",
                "        self.time = np.full(self.n, 30.0, dtype=np.float32)\n",
                "        self.done = np.zeros(self.n, dtype=bool)\n",
                "        self.kick = np.zeros((self.n, 2), dtype=bool)\n",
                "    \n",
                "    def step(self, a1, a2):\n",
                "        \"\"\"Step all environments. Returns events and done flags.\"\"\"\n",
                "        # Apply movement (fully vectorized)\n",
                "        m1, m2 = self.MOVES[a1], self.MOVES[a2]\n",
                "        self.p1[:, 2:4] += m1 * 2\n",
                "        self.p2[:, 2:4] += m2 * 2\n",
                "        \n",
                "        # Record kicks\n",
                "        self.kick[:, 0] = (a1 == 8)\n",
                "        self.kick[:, 1] = (a2 == 8)\n",
                "        \n",
                "        # Clamp speeds\n",
                "        for p in [self.p1, self.p2]:\n",
                "            speed = np.sqrt(p[:, 2]**2 + p[:, 3]**2)\n",
                "            fast = speed > 4\n",
                "            if np.any(fast):\n",
                "                scale = 4.0 / np.maximum(speed[fast], 1e-8)\n",
                "                p[fast, 2] *= scale\n",
                "                p[fast, 3] *= scale\n",
                "        \n",
                "        # Update positions\n",
                "        self.p1[:, :2] += self.p1[:, 2:4]\n",
                "        self.p2[:, :2] += self.p2[:, 2:4]\n",
                "        self.ball[:, :2] += self.ball[:, 2:4]\n",
                "        \n",
                "        # Apply friction\n",
                "        self.p1[:, 2:4] *= 0.85\n",
                "        self.p2[:, 2:4] *= 0.85\n",
                "        self.ball[:, 2:4] *= 0.98\n",
                "        \n",
                "        # Constrain players\n",
                "        self.p1[:, 0] = np.clip(self.p1[:, 0], 25, self.W - 25)\n",
                "        self.p1[:, 1] = np.clip(self.p1[:, 1], 25, self.H - 25)\n",
                "        self.p2[:, 0] = np.clip(self.p2[:, 0], 25, self.W - 25)\n",
                "        self.p2[:, 1] = np.clip(self.p2[:, 1], 25, self.H - 25)\n",
                "        \n",
                "        # Ball wall bounces\n",
                "        top = self.ball[:, 1] < 12\n",
                "        bot = self.ball[:, 1] > self.H - 12\n",
                "        self.ball[top, 1] = 12\n",
                "        self.ball[bot, 1] = self.H - 12\n",
                "        self.ball[top | bot, 3] *= -0.8\n",
                "        \n",
                "        # Side walls (outside goal area)\n",
                "        in_goal = (self.ball[:, 1] > 150) & (self.ball[:, 1] < 270)\n",
                "        left = (self.ball[:, 0] < 12) & ~in_goal\n",
                "        right = (self.ball[:, 0] > self.W - 12) & ~in_goal\n",
                "        self.ball[left, 0] = 12\n",
                "        self.ball[right, 0] = self.W - 12\n",
                "        self.ball[left | right, 2] *= -0.8\n",
                "        \n",
                "        # Player-ball collisions (vectorized)\n",
                "        for i, p in enumerate([self.p1, self.p2]):\n",
                "            dx = self.ball[:, 0] - p[:, 0]\n",
                "            dy = self.ball[:, 1] - p[:, 1]\n",
                "            dist = np.sqrt(dx**2 + dy**2)\n",
                "            hit = (dist > 0) & (dist < 37)\n",
                "            if np.any(hit):\n",
                "                nx = dx[hit] / dist[hit]\n",
                "                ny = dy[hit] / dist[hit]\n",
                "                self.ball[hit, 0] = p[hit, 0] + nx * 37\n",
                "                self.ball[hit, 1] = p[hit, 1] + ny * 37\n",
                "                power = np.where(self.kick[hit, i], 12.0, 6.0)\n",
                "                self.ball[hit, 2] = nx * power + p[hit, 2] * 0.5\n",
                "                self.ball[hit, 3] = ny * power + p[hit, 3] * 0.5\n",
                "        \n",
                "        # Check goals\n",
                "        events = np.zeros(self.n, dtype=np.int32)  # 0=none, 1=p1 scored, -1=p2 scored\n",
                "        \n",
                "        p2_goal = in_goal & (self.ball[:, 0] < 0)\n",
                "        p1_goal = in_goal & (self.ball[:, 0] > self.W)\n",
                "        \n",
                "        if np.any(p2_goal):\n",
                "            self.scores[p2_goal, 1] += 1\n",
                "            events[p2_goal] = -1\n",
                "            self._reset_pos(p2_goal)\n",
                "        \n",
                "        if np.any(p1_goal):\n",
                "            self.scores[p1_goal, 0] += 1\n",
                "            events[p1_goal] = 1\n",
                "            self._reset_pos(p1_goal)\n",
                "        \n",
                "        # Update time\n",
                "        self.time -= 1/60\n",
                "        self.done = self.time <= 0\n",
                "        \n",
                "        return events, self.done.copy()\n",
                "    \n",
                "    def _reset_pos(self, mask):\n",
                "        self.p1[mask, :2] = [120, 210]\n",
                "        self.p2[mask, :2] = [600, 210]\n",
                "        self.ball[mask] = [360, 210, 0, 0]\n",
                "    \n",
                "    def get_state(self, player=1):\n",
                "        \"\"\"Get observation for player 1 or 2.\"\"\"\n",
                "        if player == 1:\n",
                "            p, o = self.p1, self.p2\n",
                "        else:\n",
                "            p, o = self.p2, self.p1\n",
                "        \n",
                "        dist = np.sqrt((p[:, 0] - self.ball[:, 0])**2 + \n",
                "                      (p[:, 1] - self.ball[:, 1])**2) / 830\n",
                "        \n",
                "        return np.column_stack([\n",
                "            p[:, 0] / self.W,\n",
                "            p[:, 1] / self.H,\n",
                "            self.ball[:, 0] / self.W,\n",
                "            self.ball[:, 1] / self.H,\n",
                "            np.clip(self.ball[:, 2] / 15, -1, 1),\n",
                "            np.clip(self.ball[:, 3] / 15, -1, 1),\n",
                "            o[:, 0] / self.W,\n",
                "            o[:, 1] / self.H,\n",
                "            dist,\n",
                "            np.zeros(self.n),  # angle placeholder\n",
                "            np.zeros(self.n),  # goal dist placeholder\n",
                "            np.zeros(self.n),  # goal angle placeholder\n",
                "        ]).astype(np.float32)\n",
                "\n",
                "print('âœ… Game engine ready')"
            ],
            "metadata": {
                "id": "game"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================\n",
                "# SIMPLE AI OPPONENT (fully vectorized)\n",
                "# ============================================================\n",
                "def ai_actions(states):\n",
                "    \"\"\"Vectorized AI that chases ball and kicks when close.\"\"\"\n",
                "    n = len(states)\n",
                "    px, py = states[:, 0], states[:, 1]\n",
                "    bx, by = states[:, 2], states[:, 3]\n",
                "    dist = states[:, 8]\n",
                "    \n",
                "    # Default: stay\n",
                "    actions = np.full(n, 9, dtype=np.int32)\n",
                "    \n",
                "    # Kick when very close\n",
                "    actions[dist < 0.04] = 8\n",
                "    \n",
                "    # Chase ball otherwise\n",
                "    dx = bx - px\n",
                "    dy = by - py\n",
                "    \n",
                "    # Encode movement as single number for vectorized lookup\n",
                "    mx = (dx > 0.02).astype(np.int32) - (dx < -0.02).astype(np.int32)  # 1, 0, -1\n",
                "    my = (dy > 0.02).astype(np.int32) - (dy < -0.02).astype(np.int32)  # 1, 0, -1\n",
                "    \n",
                "    # Map (mx, my) to action index\n",
                "    # (0,-1)->0, (0,1)->1, (-1,0)->2, (1,0)->3\n",
                "    # (-1,-1)->4, (1,-1)->5, (-1,1)->6, (1,1)->7\n",
                "    chase = dist >= 0.04\n",
                "    \n",
                "    actions[chase & (mx == 0) & (my == -1)] = 0\n",
                "    actions[chase & (mx == 0) & (my == 1)] = 1\n",
                "    actions[chase & (mx == -1) & (my == 0)] = 2\n",
                "    actions[chase & (mx == 1) & (my == 0)] = 3\n",
                "    actions[chase & (mx == -1) & (my == -1)] = 4\n",
                "    actions[chase & (mx == 1) & (my == -1)] = 5\n",
                "    actions[chase & (mx == -1) & (my == 1)] = 6\n",
                "    actions[chase & (mx == 1) & (my == 1)] = 7\n",
                "    \n",
                "    return actions\n",
                "\n",
                "print('âœ… AI ready')"
            ],
            "metadata": {
                "id": "ai"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================\n",
                "# DUELING DQN MODEL\n",
                "# ============================================================\n",
                "def create_model():\n",
                "    \"\"\"Create Dueling DQN matching browser architecture.\"\"\"\n",
                "    inp = layers.Input(shape=(12,))\n",
                "    x = layers.Dense(256, activation='relu', kernel_initializer='he_normal')(inp)\n",
                "    x = layers.Dense(256, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    x = layers.Dense(128, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    \n",
                "    # Value stream\n",
                "    v = layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    v = layers.Dense(1, kernel_initializer='he_normal')(v)\n",
                "    \n",
                "    # Advantage stream\n",
                "    a = layers.Dense(64, activation='relu', kernel_initializer='he_normal')(x)\n",
                "    a = layers.Dense(10, kernel_initializer='he_normal')(a)\n",
                "    \n",
                "    # Combine: Q = V + (A - mean(A))\n",
                "    mean_a = layers.Lambda(lambda t: tf.reduce_mean(t, axis=1, keepdims=True))(a)\n",
                "    q = layers.Add()([v, layers.Subtract()([a, mean_a])])\n",
                "    \n",
                "    model = keras.Model(inp, q)\n",
                "    return model\n",
                "\n",
                "# Compiled functions for GPU efficiency\n",
                "@tf.function(reduce_retracing=True)\n",
                "def predict_q(model, states):\n",
                "    return model(states, training=False)\n",
                "\n",
                "@tf.function(reduce_retracing=True)\n",
                "def train_step(model, optimizer, states, targets):\n",
                "    with tf.GradientTape() as tape:\n",
                "        q = model(states, training=True)\n",
                "        loss = tf.reduce_mean(tf.square(q - targets))\n",
                "    grads = tape.gradient(loss, model.trainable_variables)\n",
                "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
                "    return loss\n",
                "\n",
                "print('âœ… Model ready')"
            ],
            "metadata": {
                "id": "model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================\n",
                "# EFFICIENT REPLAY BUFFER (NumPy-based)\n",
                "# ============================================================\n",
                "class ReplayBuffer:\n",
                "    def __init__(self, capacity):\n",
                "        self.capacity = capacity\n",
                "        self.states = np.zeros((capacity, 12), dtype=np.float32)\n",
                "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
                "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
                "        self.next_states = np.zeros((capacity, 12), dtype=np.float32)\n",
                "        self.dones = np.zeros(capacity, dtype=bool)\n",
                "        self.ptr = 0\n",
                "        self.size = 0\n",
                "    \n",
                "    def add(self, s, a, r, s2, d):\n",
                "        \"\"\"Add batch of transitions.\"\"\"\n",
                "        n = len(s)\n",
                "        idx = np.arange(self.ptr, self.ptr + n) % self.capacity\n",
                "        self.states[idx] = s\n",
                "        self.actions[idx] = a\n",
                "        self.rewards[idx] = r\n",
                "        self.next_states[idx] = s2\n",
                "        self.dones[idx] = d\n",
                "        self.ptr = (self.ptr + n) % self.capacity\n",
                "        self.size = min(self.size + n, self.capacity)\n",
                "    \n",
                "    def sample(self, batch_size):\n",
                "        idx = np.random.choice(self.size, batch_size, replace=False)\n",
                "        return (self.states[idx], self.actions[idx], self.rewards[idx],\n",
                "                self.next_states[idx], self.dones[idx])\n",
                "\n",
                "print('âœ… Buffer ready')"
            ],
            "metadata": {
                "id": "buffer"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================\n",
                "# REWARD FUNCTION (vectorized)\n",
                "# ============================================================\n",
                "def compute_rewards(game, events, prev_dist):\n",
                "    \"\"\"Calculate rewards for all environments.\"\"\"\n",
                "    n = game.n\n",
                "    p, b, o = game.p1, game.ball, game.p2\n",
                "    \n",
                "    dist = np.sqrt((p[:, 0] - b[:, 0])**2 + (p[:, 1] - b[:, 1])**2)\n",
                "    r = np.zeros(n, dtype=np.float32)\n",
                "    \n",
                "    # Goal rewards\n",
                "    r[events == 1] += 500   # Scored\n",
                "    r[events == -1] -= 300  # Conceded\n",
                "    \n",
                "    # Proximity reward (0-5 based on distance)\n",
                "    r += (1 - dist / 830) * 5\n",
                "    \n",
                "    # Ball possession bonus\n",
                "    r[dist < 40] += 10\n",
                "    \n",
                "    # Approach reward\n",
                "    if prev_dist is not None:\n",
                "        delta = prev_dist - dist\n",
                "        r += delta * 0.5\n",
                "        r[delta > 2] += 3  # Fast approach bonus\n",
                "    \n",
                "    # Movement rewards\n",
                "    speed = np.sqrt(p[:, 2]**2 + p[:, 3]**2)\n",
                "    r[(speed < 0.5) & (dist > 50)] -= 8  # Standing still penalty\n",
                "    r[speed > 1] += 1  # Movement bonus\n",
                "    \n",
                "    # Attack rewards\n",
                "    r[(b[:, 2] > 2) & (dist < 80)] += 8  # Ball moving toward goal\n",
                "    r[np.abs(b[:, 0] - 720) < 100] += 5  # Ball near goal\n",
                "    \n",
                "    # Penalties\n",
                "    corner = ((p[:, 0] < 80) | (p[:, 0] > 640)) & ((p[:, 1] < 80) | (p[:, 1] > 340))\n",
                "    r[corner] -= 5\n",
                "    r[corner & (dist > 100)] -= 5\n",
                "    \n",
                "    r[dist > 300] -= 5\n",
                "    r[(dist > 200) & (dist <= 300)] -= 3\n",
                "    r[(dist > 150) & (dist <= 200)] -= 1\n",
                "    \n",
                "    # Opponent closer penalty\n",
                "    o_dist = np.sqrt((o[:, 0] - b[:, 0])**2 + (o[:, 1] - b[:, 1])**2)\n",
                "    r[(o_dist < dist) & (dist > 60)] -= 2\n",
                "    \n",
                "    r -= 0.1  # Time penalty\n",
                "    \n",
                "    return r, dist\n",
                "\n",
                "print('âœ… Rewards ready')"
            ],
            "metadata": {
                "id": "rewards"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ============================================================\n",
                "# MAIN TRAINING LOOP\n",
                "# ============================================================\n",
                "def train():\n",
                "    print('=' * 60)\n",
                "    print('ðŸš€ RL Football GPU Training')\n",
                "    print('=' * 60)\n",
                "    \n",
                "    # Initialize\n",
                "    game = Game(N_ENVS)\n",
                "    model = create_model()\n",
                "    target = create_model()\n",
                "    target.set_weights(model.get_weights())\n",
                "    optimizer = keras.optimizers.Adam(LR)\n",
                "    buffer = ReplayBuffer(BUFFER_SIZE)\n",
                "    \n",
                "    eps = 1.0\n",
                "    train_steps = 0\n",
                "    total_ep = 0\n",
                "    step_count = 0\n",
                "    stats = {'W': 0, 'L': 0, 'D': 0, 'goals': 0}\n",
                "    t0 = time.time()\n",
                "    \n",
                "    print(f'Training {EPISODES} episodes with {N_ENVS} parallel envs...')\n",
                "    print('=' * 60)\n",
                "    \n",
                "    while total_ep < EPISODES:\n",
                "        game.reset()\n",
                "        prev_dist = None\n",
                "        \n",
                "        while not np.all(game.done):\n",
                "            step_count += 1\n",
                "            active = ~game.done\n",
                "            \n",
                "            # Get states\n",
                "            s1 = game.get_state(1)\n",
                "            s2 = game.get_state(2)\n",
                "            \n",
                "            # Select actions\n",
                "            a1 = np.random.randint(0, 10, N_ENVS)  # Start with random\n",
                "            exploit = np.random.random(N_ENVS) >= eps\n",
                "            if np.any(exploit):\n",
                "                q = predict_q(model, tf.constant(s1[exploit])).numpy()\n",
                "                a1[exploit] = np.argmax(q, axis=1)\n",
                "            \n",
                "            a2 = ai_actions(s2)\n",
                "            \n",
                "            # Step environment\n",
                "            events, dones = game.step(a1, a2)\n",
                "            \n",
                "            # Calculate rewards\n",
                "            rewards, prev_dist = compute_rewards(game, events, prev_dist)\n",
                "            \n",
                "            # Get next states\n",
                "            next_s1 = game.get_state(1)\n",
                "            \n",
                "            # Store transitions (only active envs)\n",
                "            buffer.add(s1[active], a1[active], rewards[active], \n",
                "                       next_s1[active], dones[active])\n",
                "            \n",
                "            # Track goals\n",
                "            stats['goals'] += np.sum(events != 0)\n",
                "            \n",
                "            # Training step\n",
                "            if buffer.size >= 1000 and step_count % TRAIN_EVERY == 0:\n",
                "                s, a, r, s2, d = buffer.sample(BATCH_SIZE)\n",
                "                \n",
                "                # Double DQN target calculation\n",
                "                q_next = predict_q(model, tf.constant(s2)).numpy()\n",
                "                q_target = predict_q(target, tf.constant(s2)).numpy()\n",
                "                best_a = np.argmax(q_next, axis=1)\n",
                "                next_q = q_target[np.arange(BATCH_SIZE), best_a]\n",
                "                \n",
                "                # Build targets\n",
                "                targets = predict_q(model, tf.constant(s)).numpy()\n",
                "                targets[np.arange(BATCH_SIZE), a] = r + GAMMA * next_q * ~d\n",
                "                \n",
                "                # Train\n",
                "                train_step(model, optimizer, tf.constant(s), tf.constant(targets))\n",
                "                train_steps += 1\n",
                "                \n",
                "                # Update target network\n",
                "                if train_steps % TARGET_UPDATE == 0:\n",
                "                    target.set_weights(model.get_weights())\n",
                "        \n",
                "        # Episode complete - count outcomes\n",
                "        for i in range(N_ENVS):\n",
                "            if game.scores[i, 0] > game.scores[i, 1]:\n",
                "                stats['W'] += 1\n",
                "            elif game.scores[i, 1] > game.scores[i, 0]:\n",
                "                stats['L'] += 1\n",
                "            else:\n",
                "                stats['D'] += 1\n",
                "        \n",
                "        total_ep += N_ENVS\n",
                "        \n",
                "        # Decay epsilon\n",
                "        eps = max(EPS_MIN, eps * (EPS_DECAY ** N_ENVS))\n",
                "        \n",
                "        # Progress logging\n",
                "        if total_ep % (50 * N_ENVS) < N_ENVS:\n",
                "            elapsed = time.time() - t0\n",
                "            eps_rate = total_ep / elapsed if elapsed > 0 else 0\n",
                "            eta = (EPISODES - total_ep) / eps_rate if eps_rate > 0 else 0\n",
                "            eta_str = f'{eta/3600:.1f}h' if eta > 3600 else f'{eta/60:.0f}m'\n",
                "            print(f'Ep {total_ep:,}/{EPISODES:,} | Îµ:{eps:.3f} | '\n",
                "                  f'W:{stats[\"W\"]} L:{stats[\"L\"]} D:{stats[\"D\"]} | '\n",
                "                  f'{eps_rate:.1f}/s | ETA:{eta_str}')\n",
                "        \n",
                "        # Save checkpoint\n",
                "        if total_ep % SAVE_EVERY < N_ENVS:\n",
                "            save_weights(model, eps, train_steps, f'weights_{total_ep}.json')\n",
                "    \n",
                "    # Final save\n",
                "    save_weights(model, eps, train_steps, 'trained.json')\n",
                "    \n",
                "    print('=' * 60)\n",
                "    print('âœ… Training Complete!')\n",
                "    print(f'Final: W:{stats[\"W\"]} L:{stats[\"L\"]} D:{stats[\"D\"]}')\n",
                "    print(f'Total goals: {stats[\"goals\"]}')\n",
                "    print(f'Time: {(time.time() - t0) / 3600:.2f} hours')\n",
                "    print('=' * 60)\n",
                "    \n",
                "    return model, stats\n",
                "\n",
                "def save_weights(model, eps, steps, filename):\n",
                "    weights = [{'shape': list(w.shape), 'data': w.flatten().tolist()} \n",
                "               for w in model.get_weights()]\n",
                "    agent = {'weights': weights, 'epsilon': float(eps), 'trainStepCount': int(steps)}\n",
                "    data = {\n",
                "        'version': 2,\n",
                "        'aiType': 'dqn',\n",
                "        'blipAgent': agent,\n",
                "        'bloopAgent': agent,\n",
                "        'blip': agent,\n",
                "        'bloop': agent\n",
                "    }\n",
                "    with open(filename, 'w') as f:\n",
                "        json.dump(data, f)\n",
                "    print(f'ðŸ’¾ Saved {filename}')\n",
                "\n",
                "print('âœ… Training function ready - Run next cell to start!')"
            ],
            "metadata": {
                "id": "train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ðŸš€ START TRAINING\n",
                "model, stats = train()"
            ],
            "metadata": {
                "id": "run"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download trained weights\n",
                "from google.colab import files\n",
                "files.download('trained.json')"
            ],
            "metadata": {
                "id": "download"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}