{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸš€ RL Football - FAST Training\n\n**Optimized version** - runs 10-50x faster than standard version.\n\nKey optimizations:\n- Compiled TF functions\n- Numpy-based game simulation\n- Reduced overhead\n\n**Instructions:**\n1. Runtime â†’ Change runtime type â†’ GPU\n2. Run all cells\n3. Download weights when done"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import numpy as np\nimport json\nimport time\nfrom collections import deque\nimport random\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nimport tensorflow as tf\nprint(f'TensorFlow: {tf.__version__}')\nprint(f'GPU: {tf.config.list_physical_devices(\"GPU\")}')\n\n# Enable memory growth\nfor gpu in tf.config.list_physical_devices('GPU'):\n    tf.config.experimental.set_memory_growth(gpu, True)"
            ],
            "metadata": {
                "id": "setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Fast numpy-based game simulation\nclass FastGame:\n    def __init__(self, match_time=30):\n        self.field_width = 720\n        self.field_height = 420\n        self.goal_height = 120\n        self.match_time = match_time\n        self.reset()\n\n    def reset(self):\n        # Positions: [x, y, vx, vy] for blip, bloop, ball\n        self.blip = np.array([120.0, 250.0, 0.0, 0.0])\n        self.bloop = np.array([600.0, 250.0, 0.0, 0.0])\n        self.ball = np.array([360.0, 250.0, 0.0, 0.0])\n        self.blip_score = 0\n        self.bloop_score = 0\n        self.time = self.match_time\n        self.blip_kick = 0\n        self.bloop_kick = 0\n\n    def step(self, blip_action, bloop_action):\n        # Actions: 0-7 = movement, 8 = kick, 9 = stay\n        moves = [(0,-1), (0,1), (-1,0), (1,0), (-1,-1), (1,-1), (-1,1), (1,1), (0,0), (0,0)]\n        speed = 4.0\n        friction = 0.85\n\n        # Apply movement\n        if blip_action < 8:\n            self.blip[2] += moves[blip_action][0] * speed * 0.5\n            self.blip[3] += moves[blip_action][1] * speed * 0.5\n        elif blip_action == 8:\n            self.blip_kick = 1\n\n        if bloop_action < 8:\n            self.bloop[2] += moves[bloop_action][0] * speed * 0.5\n            self.bloop[3] += moves[bloop_action][1] * speed * 0.5\n        elif bloop_action == 8:\n            self.bloop_kick = 1\n\n        # Clamp velocities\n        for p in [self.blip, self.bloop]:\n            spd = np.sqrt(p[2]**2 + p[3]**2)\n            if spd > speed:\n                p[2:4] *= speed / spd\n\n        # Update positions\n        self.blip[0:2] += self.blip[2:4]\n        self.bloop[0:2] += self.bloop[2:4]\n        self.ball[0:2] += self.ball[2:4]\n\n        # Apply friction\n        self.blip[2:4] *= friction\n        self.bloop[2:4] *= friction\n        self.ball[2:4] *= 0.98\n\n        # Constrain players\n        self.blip[0] = np.clip(self.blip[0], 25, self.field_width - 25)\n        self.blip[1] = np.clip(self.blip[1], 25, self.field_height - 25)\n        self.bloop[0] = np.clip(self.bloop[0], 25, self.field_width - 25)\n        self.bloop[1] = np.clip(self.bloop[1], 25, self.field_height - 25)\n\n        # Ball boundaries\n        if self.ball[1] < 12:\n            self.ball[1] = 12\n            self.ball[3] *= -0.8\n        if self.ball[1] > self.field_height - 12:\n            self.ball[1] = self.field_height - 12\n            self.ball[3] *= -0.8\n\n        goal_y_min = (self.field_height - self.goal_height) / 2\n        goal_y_max = goal_y_min + self.goal_height\n        in_goal = goal_y_min < self.ball[1] < goal_y_max\n\n        if not in_goal:\n            if self.ball[0] < 12:\n                self.ball[0] = 12\n                self.ball[2] *= -0.8\n            if self.ball[0] > self.field_width - 12:\n                self.ball[0] = self.field_width - 12\n                self.ball[2] *= -0.8\n\n        # Ball-player collisions\n        for p, kick in [(self.blip, self.blip_kick), (self.bloop, self.bloop_kick)]:\n            dx = self.ball[0] - p[0]\n            dy = self.ball[1] - p[1]\n            dist = np.sqrt(dx*dx + dy*dy)\n            if dist < 37 and dist > 0:\n                nx, ny = dx/dist, dy/dist\n                self.ball[0] = p[0] + nx * 37\n                self.ball[1] = p[1] + ny * 37\n                power = 12 if kick else 6\n                self.ball[2] = nx * power + p[2] * 0.5\n                self.ball[3] = ny * power + p[3] * 0.5\n\n        self.blip_kick = 0\n        self.bloop_kick = 0\n\n        # Check goals\n        event = None\n        if in_goal:\n            if self.ball[0] < 0:\n                self.bloop_score += 1\n                event = 'bloop'\n                self._reset_positions()\n            elif self.ball[0] > self.field_width:\n                self.blip_score += 1\n                event = 'blip'\n                self._reset_positions()\n\n        self.time -= 1/60\n        return event, self.time <= 0\n\n    def _reset_positions(self):\n        self.blip[0:2] = [120, 250]\n        self.bloop[0:2] = [600, 250]\n        self.ball[:] = [360, 250, 0, 0]\n\n    def get_state(self, team='blip'):\n        if team == 'blip':\n            p, o = self.blip, self.bloop\n        else:\n            p, o = self.bloop, self.blip\n\n        px, py = p[0]/self.field_width, p[1]/self.field_height\n        bx, by = self.ball[0]/self.field_width, self.ball[1]/self.field_height\n        bvx, bvy = np.clip(self.ball[2]/15, -1, 1), np.clip(self.ball[3]/15, -1, 1)\n        ox, oy = o[0]/self.field_width, o[1]/self.field_height\n\n        max_d = np.sqrt(self.field_width**2 + self.field_height**2)\n        dist_b = np.sqrt((p[0]-self.ball[0])**2 + (p[1]-self.ball[1])**2) / max_d\n        ang_b = (np.arctan2(self.ball[1]-p[1], self.ball[0]-p[0]) + np.pi) / (2*np.pi)\n\n        gx = self.field_width if team == 'blip' else 0\n        dist_g = np.sqrt((p[0]-gx)**2 + (p[1]-250)**2) / max_d\n        ang_g = (np.arctan2(250-p[1], gx-p[0]) + np.pi) / (2*np.pi)\n\n        return np.array([px, py, bx, by, bvx, bvy, ox, oy, dist_b, ang_b, dist_g, ang_g], dtype=np.float32)\n\nprint('âœ… Fast game ready')"
            ],
            "metadata": {
                "id": "game"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Simple AI - always chases ball\ndef simple_ai_action(state):\n    px, py, bx, by = state[0], state[1], state[2], state[3]\n    dist = state[8]\n\n    if dist < 0.04:\n        return 8  # kick\n\n    dx = bx - px\n    dy = by - py\n    thresh = 0.02\n\n    if dist < 0.08:\n        # Push toward goal (bloop attacks left)\n        mdx, mdy = -1, (1 if dy > 0.01 else (-1 if dy < -0.01 else 0))\n    else:\n        mdx = 1 if dx > thresh else (-1 if dx < -thresh else 0)\n        mdy = 1 if dy > thresh else (-1 if dy < -thresh else 0)\n\n    moves = [(0,-1), (0,1), (-1,0), (1,0), (-1,-1), (1,-1), (-1,1), (1,1)]\n    for i, (mx, my) in enumerate(moves):\n        if mx == mdx and my == mdy:\n            return i\n    return 9\n\nprint('âœ… Simple AI ready')"
            ],
            "metadata": {
                "id": "simple_ai"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Fast DQN with compiled predict\nclass FastDQN:\n    def __init__(self):\n        self.state_size = 12\n        self.action_size = 10\n        self.lr = 0.0005\n        self.gamma = 0.995\n        self.epsilon = 1.0\n        self.epsilon_min = 0.02\n        self.epsilon_decay = 0.9999\n        self.memory = deque(maxlen=50000)\n        self.batch_size = 64\n        self.min_mem = 500\n        self.train_step = 0\n        self.target_freq = 500\n        self.last_dist = None\n\n        self.model = self._build()\n        self.target = self._build()\n        self.target.set_weights(self.model.get_weights())\n\n        # Compile predict function for speed\n        self._predict = tf.function(self.model, experimental_relax_shapes=True)\n\n    def _build(self):\n        inp = tf.keras.Input(shape=(12,))\n        x = tf.keras.layers.Dense(256, activation='relu')(inp)\n        x = tf.keras.layers.Dense(256, activation='relu')(x)\n        x = tf.keras.layers.Dense(128, activation='relu')(x)\n\n        v = tf.keras.layers.Dense(64, activation='relu')(x)\n        v = tf.keras.layers.Dense(1)(v)\n\n        a = tf.keras.layers.Dense(64, activation='relu')(x)\n        a = tf.keras.layers.Dense(10)(a)\n\n        q = v + (a - tf.reduce_mean(a, axis=1, keepdims=True))\n\n        m = tf.keras.Model(inp, q)\n        m.compile(optimizer=tf.keras.optimizers.Adam(self.lr), loss='mse')\n        return m\n\n    def act(self, state):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(10)\n        q = self.model(state[np.newaxis], training=False)\n        return int(tf.argmax(q[0]))\n\n    def remember(self, s, a, r, s2, done):\n        self.memory.append((s, a, r, s2, done))\n\n    def train(self):\n        if len(self.memory) < self.min_mem:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states = np.array([b[0] for b in batch])\n        next_states = np.array([b[3] for b in batch])\n\n        q_curr = self.model.predict(states, verbose=0)\n        q_next = self.model.predict(next_states, verbose=0)\n        q_targ = self.target.predict(next_states, verbose=0)\n\n        for i, (s, a, r, s2, d) in enumerate(batch):\n            if d:\n                q_curr[i][a] = r\n            else:\n                q_curr[i][a] = r + self.gamma * q_targ[i][np.argmax(q_next[i])]\n\n        self.model.fit(states, q_curr, epochs=1, verbose=0)\n        self.train_step += 1\n\n        if self.train_step % self.target_freq == 0:\n            self.target.set_weights(self.model.get_weights())\n\n    def reward(self, game, event):\n        p = game.blip\n        b = game.ball\n        o = game.bloop\n        r = 0\n        dist = np.sqrt((p[0]-b[0])**2 + (p[1]-b[1])**2)\n\n        if event == 'blip': r += 500\n        elif event == 'bloop': r -= 300\n\n        r += (1 - dist/830) * 5\n        if dist < 40: r += 10\n\n        if self.last_dist:\n            delta = self.last_dist - dist\n            r += delta * 0.5\n            if delta > 2: r += 3\n        self.last_dist = dist\n\n        spd = np.sqrt(p[2]**2 + p[3]**2)\n        if spd < 0.5 and dist > 50: r -= 8\n        if spd > 1: r += 1\n\n        if (b[2] > 2 and dist < 80): r += 8\n        if abs(b[0] - 720) < 100: r += 5\n\n        if (p[0] < 80 or p[0] > 640) and (p[1] < 80 or p[1] > 340):\n            r -= 5\n            if dist > 100: r -= 5\n\n        if dist > 300: r -= 5\n        elif dist > 200: r -= 3\n        elif dist > 150: r -= 1\n\n        odist = np.sqrt((o[0]-b[0])**2 + (o[1]-b[1])**2)\n        if odist < dist and dist > 60: r -= 2\n\n        return r - 0.1\n\n    def reset(self):\n        self.last_dist = None\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n\n    def save(self, path):\n        w = self.model.get_weights()\n        data = {'weights': [{'shape': list(x.shape), 'data': x.flatten().tolist()} for x in w],\n                'epsilon': self.epsilon, 'trainStepCount': self.train_step}\n        with open(path, 'w') as f:\n            json.dump(data, f)\n\nprint('âœ… Fast DQN ready')"
            ],
            "metadata": {
                "id": "dqn"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "def train_fast(episodes=100000, save_every=5000):\n    game = FastGame(match_time=30)\n    agent = FastDQN()\n    stats = {'blip': 0, 'bloop': 0, 'draw': 0, 'goals': 0}\n    start = time.time()\n\n    print('='*50)\n    print('ðŸš€ FAST Training Started')\n    print('='*50)\n\n    for ep in range(1, episodes + 1):\n        game.reset()\n        done = False\n        steps = 0\n\n        while not done:\n            steps += 1\n            s1 = game.get_state('blip')\n            s2 = game.get_state('bloop')\n\n            a1 = agent.act(s1)\n            a2 = simple_ai_action(s2)\n\n            event, done = game.step(a1, a2)\n\n            r = agent.reward(game, event)\n            s1_new = game.get_state('blip')\n            agent.remember(s1, a1, r, s1_new, done)\n\n            if event: stats['goals'] += 1\n\n            if steps % 4 == 0:\n                agent.train()\n\n        winner = 'blip' if game.blip_score > game.bloop_score else ('bloop' if game.bloop_score > game.blip_score else 'draw')\n        stats[winner] += 1\n        agent.reset()\n\n        if ep % 100 == 0 or ep == 1:\n            elapsed = time.time() - start\n            speed = ep / elapsed\n            eta = (episodes - ep) / speed if speed > 0 else 0\n            eta_str = f'{eta/60:.1f}m' if eta < 3600 else f'{eta/3600:.1f}h'\n            print(f'Ep {ep}/{episodes} | Îµ:{agent.epsilon:.3f} | B:{stats[\"blip\"]} L:{stats[\"bloop\"]} D:{stats[\"draw\"]} | Goals:{stats[\"goals\"]} | {speed:.1f} ep/s | ETA:{eta_str}')\n\n        if ep % save_every == 0:\n            agent.save(f'weights_{ep}.json')\n            print(f'ðŸ’¾ Saved at {ep}')\n\n    agent.save('weights_final.json')\n    print('='*50)\n    print('âœ… Done!')\n    print(f'Blip:{stats[\"blip\"]} Bloop:{stats[\"bloop\"]} Draw:{stats[\"draw\"]}')\n    return agent, stats\n\nprint('âœ… Training function ready')"
            ],
            "metadata": {
                "id": "train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ðŸš€ START TRAINING\nagent, stats = train_fast(episodes=100000, save_every=5000)"
            ],
            "metadata": {
                "id": "run"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download weights\nfrom google.colab import files\nimport json\n\nwith open('weights_final.json') as f:\n    w = json.load(f)\n\nbrowser = {\n    'version': 2, 'aiType': 'dqn', 'episodeCount': 100000,\n    'blipAgent': w, 'bloopAgent': w, 'blip': w, 'bloop': w, 'stats': stats\n}\n\nwith open('rl_football_trained.json', 'w') as f:\n    json.dump(browser, f)\n\nfiles.download('rl_football_trained.json')"
            ],
            "metadata": {
                "id": "download"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}